{"data":{"jobs":{"edges":[{"node":{"frontmatter":{"title":"Live Demonstration: Real-Time Implementation of Proto-Object Based Visual Saliency Model","company":"ISCAS 2019","location":null,"range":null,"url":null},"html":"<p>Real-time Implementation ofProto-Object Based Visual Saliency Model</p>\n<p>We  demonstrate  a  real-time  implementation  of  a  pro-toobject  based  neuromorphic  visual  saliency  model on an  embedded  processing  board  .  Visual  saliency  models  aredifficult  to  implement  in  hardware  for  real-time  applicationsdue  to  their  computational  complexity.  The  conventional  im-plementation  is  not  optimal  because  of  the  requirement  of  alarge number of convolution operations for filtering on several feature  channels  across  multiple  image  pyramids.  Our  current  implementation  considers  the  dynamic  temporal  motion change  by  convoluting  along  time  efficiently  by  parallelly processing  them.  We  have  implemented  the  model  on  an NVIDIA  Jetson  TX1  board,  which  has  NVIDIAMaxwell  GPU  with  256  NVIDIA  CUDA  Cores, hosted  onan  Ubuntu  environment.  The  board  has  a  5  MP  fixed  focus MIPI  CSI  camera  through  which  the  frames  are  fetched using a Quad-core ARM Cortex-A57 MPCore Processor with 4 GB LPDDR4  Memory.  The  camera  module  fetches  theframes to the application for processing, and the result is thendisplayed  through  the  HDMI  port.  The  application  is  writtenin  Tensorflow,  Cuda,  and  Python  and  uses  several  Python libraries. For further analysis, the user can also save the output onto a file.</p>"}},{"node":{"frontmatter":{"title":"N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces","company":"ISCAS 2019","location":null,"range":null,"url":null},"html":"<p>N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces.</p>\n<p>In  recent  years,  a  new  generation  of  low-power,neuromorphic, event-based vision sensors has been gaining pop-ularity  for  their  very  low  latency  and  data  sparsity.  Thoughthe conventional frame-based cameras have advanced in a lot ofways,  they  suffer  from  data  redundancy  and  temporal  latency.The bio-inspired artificial retinas eliminate the data redundancyby  capturing  only  the  change  in  illumination  at  each  pixel  andasynchronously  communicating  in  binary  spikes.  In  this  work,we  propose  a  system  to  achieve  the  task  of  human  activityrecognition  based  on  the  event-based  camera  data.  We  showthat  such  tasks,  which  generally  need  high  frame  rate  sensorsfor  accurate  predictions,  can  be  achieved  by  adapting  existingcomputer vision techniques to the spiking domain. We used eventmemory surfaces to make the sparse event data compatible withdeep  convolutional  neural  networks  (CNNs).  We  leverage  uponthe recent advances in deep convolutional networks based videoanalysis  and  adapt  such  frameworks  onto  the  neuromorphicdomain.  We  also  provide  the  community  with  a  new  datasetconsisting of five categories of human activities captured in realworld  without  any  simulations.  We  achieved  an  accuracy  of94.3%  using  event  memory  surfaces  on  our  activity  recognitiondataset.</p>"}},{"node":{"frontmatter":{"title":"A Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware","company":"ICIP 2021","location":null,"range":null,"url":null},"html":"<p>Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware</p>\n<ul>\n<li>Object detection and localization can be possible directly in the Compressed Domain (easily upto 20x compression).</li>\n<li>Achieved SOTA 46.27% mAP(Mean Average Precision) on a GeForce GTX 1080 Ti with an inference time of 23ms.</li>\n<li>Deployed on a NVIDIA TX2 embedded board with 45.11% mAP with an inference time of 34ms.</li>\n</ul>"}}]}}}