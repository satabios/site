{"data":{"featured":{"edges":[{"node":{"frontmatter":{"title":"Object Detection and Localization in Compressive Sensing Video","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGdtizBwn//xAAaEAEAAgMBAAAAAAAAAAAAAAABAAIQESEi/9oACAEBAAEFAmup6lUS3cE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGxAAAQQDAAAAAAAAAAAAAAAAAQAQETIhMZH/2gAIAQEABj8CFuNYrMrZb//EABsQAQADAAMBAAAAAAAAAAAAAAEAESFBYXGB/9oACAEBAAE/IbCMBaxp5plpfsQYEjtluGHsFXP/2gAMAwEAAgADAAAAEKDf/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8QR//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB4QAQACAgEFAAAAAAAAAAAAAAEAESExQWFxgZGh/9oACAEBAAE/EMMBlUpvXbrETkM3gV9GIcA4yOogVTkVPrLxkSFXqjUGgKL14J//2Q=="},"images":{"fallback":{"src":"/static/2407df0d0156fe333a242e66647488bd/2b365/cs.jpg","srcSet":"/static/2407df0d0156fe333a242e66647488bd/1849b/cs.jpg 175w,\n/static/2407df0d0156fe333a242e66647488bd/3f5de/cs.jpg 350w,\n/static/2407df0d0156fe333a242e66647488bd/2b365/cs.jpg 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/2407df0d0156fe333a242e66647488bd/dae43/cs.avif 175w,\n/static/2407df0d0156fe333a242e66647488bd/d7667/cs.avif 350w,\n/static/2407df0d0156fe333a242e66647488bd/7ec1a/cs.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/2407df0d0156fe333a242e66647488bd/5d873/cs.webp 175w,\n/static/2407df0d0156fe333a242e66647488bd/26a00/cs.webp 350w,\n/static/2407df0d0156fe333a242e66647488bd/f23f0/cs.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":394}}},"tech":["Deep Learning","Computer Vision","Object Detection and Localization","Compressive Sensing","NVIDIA Jetson TX2"],"github":"https://gitlab.com/neuronics/object_detection_compressed_frames","external":"https://arxiv.org/abs/1912.08519","conference":"ICIP 2021"},"html":"<p><a>A REAL-TIME OBJECT DETECTION AND LOCALIZATION IN COMPRESSIVE SENSED VIDEO</a>\n<i>*ICIP 2021 Paper Link yet to be linked</i></p>\n<ul>\n<li>Object detection and localization can be possible directly in the Compressed Domain (easily upto 20x compression).</li>\n<li>Achieved <a>SOTA 46.27% mAP</a>(Mean Average Precision) on a GeForce GTX 1080 Ti with an inference time of 23ms.</li>\n<li>Deployed on a NVIDIA TX2 embedded board with <a>45.11% mAP</a> with an inference time of <a>34ms</a>.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Proto-Object Based Visual Saliency","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAABJ0AAASdAHeZh94AAAD8klEQVQ4y22S228TRxjFV0pf2odKqJX6L1R56FtU96WipCqoaYC0UqWKFqRe6ENFiRKpqFJFCSnFREkQQaCqUNIIJRBCLkqCSWJjx8ZOBDZxCGltry9Z27vr9e56L/bu2ruzM1M5DhSqHs3bnN98Z2YOcf3KxcvnTl453zt45uSN4asVo0YzLC+IgigVBZEpcLpRNQHEwB7e0L5b0Y77qsfGtePOUoGrEgHnwcgvb68PtC6daPH1HeIEqSSVeF4gSbJQKCiyYpomwnV1nMgQ+8SmdovYA5pey5NJldDnjuGFL8DCUXTnEF78QS4bNMMoallRKjwvyoqq64amGRjXPvs0TBD+13czr7yRefVlf4YqEczI4crI3trYAfXSO+K1zysGiMdjqVQyn6dDoRBJknZd9dkdHSRBJJqaVgjir6aXQmRKJuI3f2LHvjXmu4XRL/O3fqxa8MnGhsft9nm9/mU/SZIAALSd+2xf9v19G/vbg21tkY6P/QxbJjDGilrOUPk0latoGsb40tCFTw52dHd1957qCT94gDGGEGKMy2VRkvKSxArClqpy1ZpZhxFCEEEIoW3bGOM+59mPPmz7+qtvTp/q8d3zPIO3nbiRAiJUMwGBn1MD7u9zvtXc7GhxOFoc07dvPg8/E0KoZr0IN0wXLwy17mnd335g97vvTU7cwBgb1ZppAdMCwIaNZQLbqFrEzjmwHhxDaEMkK2qeZhi2QDMFtVyB6N9pANgAQACgZdnAhsTOJf5PaHtLVUp35iYHhgY9ft9/DASGdo2mpWRSIkk9kbBkhS0I6dQWSVK5PFdR9ehmambWFbr2OxdYxgijF2CW1dKZzWBwzeulw2ExTVOcuPJwbTUSTWaZJ4zW5vScvv5IeLOZ2bUL5XKSrqmy/BSm8wWGHr016uz/1etdkik2kYj9cfXy+f4za9G1tGgd/S3QOxygP9ibdzjW3Qt/jo7MTt+2LKsB04rAp9Lph5FwNp0sklRR4OMkGYmEU1nGsOBmKpumi/F4KhgK8yV5Ze1vTTcaf07Y2SyGsMjzWxRl16qFeAYhzDIsX+RUw5R1k8rmRFnxLAcWPN5EMj0z5xJEqfGcBMxlMQCmZdUsC9t2MUkhjE3TBJalW5Atladcnqn5pam5u2MT057AiutekBOVHRhzXOj+/e87Ozu7uh5Ho6UMPTg4cPjIkZ5TPxcEKc/L41Pz0/OL49NzYxMzs3fdk/NuQSrvwIhhNldXF12uoN8PZJl6HPMFQ4tL7lgsVq5BxQAAYVGpbMRTm4ktUS5rpg2e9obAloU4DpdKWBBgNmvqho5wFWHVgmLFtLcLu74ePeesK5NOb8/cqfo/+/bh+PUz9JAAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/99817a17252c97073620f0bec595ba2a/c48cd/proto-motion.png","srcSet":"/static/99817a17252c97073620f0bec595ba2a/e6966/proto-motion.png 175w,\n/static/99817a17252c97073620f0bec595ba2a/c6fbd/proto-motion.png 350w,\n/static/99817a17252c97073620f0bec595ba2a/c48cd/proto-motion.png 700w,\n/static/99817a17252c97073620f0bec595ba2a/891f1/proto-motion.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/99817a17252c97073620f0bec595ba2a/9e5c5/proto-motion.avif 175w,\n/static/99817a17252c97073620f0bec595ba2a/efec3/proto-motion.avif 350w,\n/static/99817a17252c97073620f0bec595ba2a/39372/proto-motion.avif 700w,\n/static/99817a17252c97073620f0bec595ba2a/411d0/proto-motion.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/99817a17252c97073620f0bec595ba2a/c54d4/proto-motion.webp 175w,\n/static/99817a17252c97073620f0bec595ba2a/fbcf8/proto-motion.webp 350w,\n/static/99817a17252c97073620f0bec595ba2a/c85d5/proto-motion.webp 700w,\n/static/99817a17252c97073620f0bec595ba2a/829b2/proto-motion.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":706}}},"tech":["Computer Vision","Visual Saliency","CUDA","Neuromorphic Proto-Object","NVIDIA Jetson TX"],"github":"https://gitlab.com/neuronics/visual_saliency","external":"https://ieeexplore.ieee.org/abstract/document/8702200","conference":"ISCAS 2019"},"html":"<p><a>Real-Time Implementation of Proto-Object Based Visual Saliency</a></p>\n<ul>\n<li>We demonstrate a real-time implementation of a proto sobject based neuromorphic visual saliency model</li>\n</ul>"}},{"node":{"frontmatter":{"title":"n-HAR","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAIAAABr+ngCAAAACXBIWXMAABJ0AAASdAHeZh94AAACv0lEQVQoz12O7UvqYBiH/YODRjXsQ5+CChSKJk2XFFKQlMXA5dZpaoFvrCmanunKt4mZupn5MrdnuvkcyjhUv0/3fXNfXD8b/BZN06zPaJoGITQMwzRNCOF4PLYsyzAMVVW//9sghJZlQQgFQXC73aFQiGVZj8cTj8f9fr/P53t4eMBxnKZplmXdbnelUoEQzufzL3gxURSFIMje3h5BEMvLy6enp9vb26urq+fn5wiCeDwer9e7tLSUSqX++2yLAqZpptPpYDCYTCZ5ng8Gg/l8Ph6P0zRdLBYpiuI4jud5kiSbzeYP82K5u7s7ODi4vLykKArDsGg0enZ2huN4LBZzuVwkSTIM43K5SqXSbzOEkCTJtbU1DMMIgkBR9OLiYnd3F0XRq6sru91OEMTx8TGCIBzH/YYty8rlcgzDcByXyWQWbROJBMuyT09Pt7e36XQ6l8uFQqHX19fftSGEDMM4HI6Tk5NAILCzs0PTtNfrdTqdLMs6nU6/30+SpMPhyOfzP8yLKRwOoyiK4/jR0dH6+nogEMAwbGNjgyRJu91+eHjo8/lWVlZ4nv8Bz+dz0zRlWS4UCvV6vdFoiKLYarVqtZooii8vLUEoSp8RBEGW5UXnD3j+GcuyZrOZpuu6DnQdGIahTSaGYYCPVZ9OpwAATdc/LgAstF9mRVFSqVSn01FHw6muD4eDyWSkTkaj0cAAaqujlCtST5Hb7bYkSYqiAAC+YE3Trq+vt7a2wuFIUx6WFTkttMu1t8dSL1fsZP/KpYpS4F/e395vbm4AANVqVZKkbDaraZqtUChsbm7u7+8LQrGpDKu93p+YIEpKRmxEE+XnZi9X6oqPnWQs8VwuQwhrtZokSZFIRFVVm6Io9/f39XrdMq1Bvz8FYDzsqZPxZDIYj/q6oTWbjVJJnE6nxmdarVa32+33+7PZ7B+DVOwI0KhOjAAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/f48068e565cf5d616dbf543a415d462b/3bd79/n-har.png","srcSet":"/static/f48068e565cf5d616dbf543a415d462b/fe11f/n-har.png 175w,\n/static/f48068e565cf5d616dbf543a415d462b/8caa9/n-har.png 350w,\n/static/f48068e565cf5d616dbf543a415d462b/3bd79/n-har.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/f48068e565cf5d616dbf543a415d462b/0b4ad/n-har.avif 175w,\n/static/f48068e565cf5d616dbf543a415d462b/bee7b/n-har.avif 350w,\n/static/f48068e565cf5d616dbf543a415d462b/19e93/n-har.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/f48068e565cf5d616dbf543a415d462b/e35d6/n-har.webp 175w,\n/static/f48068e565cf5d616dbf543a415d462b/f0765/n-har.webp 350w,\n/static/f48068e565cf5d616dbf543a415d462b/ef7dc/n-har.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":531}}},"tech":["Event Based Neurmorphic Vision","Computer Vision","Deep Learning","Human Activity Recognition"],"github":"https://gitlab.com/neuronics/n-har","external":"https://ieeexplore.ieee.org/document/8702581","conference":"ISCAS 2019"},"html":"<p><a>N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces. </a></p>\n<p>A system to achieve the task of human activity recognition based on the event-based camera data. We achieved a <a>SOTA accuracy of 94.3%</a> using event memory surfaces on our activity recognition dataset.</p>"}}]}}}