{"data":{"projects":{"edges":[{"node":{"frontmatter":{"title":"Live Demonstration: Real-Time Implementation of Proto-Object Based Visual Saliency Model","tech":["Computer Vision","CUDA","NVIDIA Jetson TX2"],"github":"https://github.com/satabios/visual_saliency_TX2","external":"https://github.com/satabios/visual_saliency_TX2"},"html":"<p>Real-Time Implementation of Proto-Object Based Visual Saliency</p>\n<ul>\n<li>We demonstrate a real-time implementation of a proto sobject based neuromorphic visual saliency model</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Neuromprhic Event Based ATIS Camera","tech":["Event Based Data","Neuromorphic Vision"],"github":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/ATIS","external":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/ATIS"},"html":"<p>REVEALS THE INVISIBLE</p>\n<p>Inspired by human vision and built on the foundation of neuromorphic engineering. PROPHESEE is the revolutionary system that gives Metavision to machines, revealing what was previously invisible to them. Capturing hyper fast and fleeting scene dynamics</p>\n<ul>\n<li>\n<p>greater than 10000 fps (equivalent temporal precision)</p>\n</li>\n<li>\n<p>Managing extreme lighting conditions</p>\n</li>\n<li>\n<p>120 dB dynamic range</p>\n</li>\n<li>\n<p>Enabling new levels of power efficiency</p>\n</li>\n<li>\n<p>&#x3C; 10mW</p>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"DAVIS Neuromprhic Event Based Camera","tech":["Event Based Data","Neuromorphic Vision"],"github":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/DAVIS","external":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/DAVIS"},"html":"<p>inivation DAVIS\nNeuromorphically inspired camera, that is based on combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is > 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at > 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum 2 pixels with 9.4% fill factor.</p>"}},{"node":{"frontmatter":{"title":"A Compressive Sensing Video dataset using Pixel-wise coded exposure","tech":["Event Based Data","Neuromorphic Vision"],"github":"https://arxiv.org/abs/1905.10054","external":"https://arxiv.org/abs/1905.10054"},"html":"<p>A Compressive Sensing Video dataset using Pixel-wise coded exposure</p>"}},{"node":{"frontmatter":{"title":"n-EAR: Neuromorphic Ego motion vehicle Activity Recognition using onboard cameras","tech":["Vehicle Activity Recognition","Computer Vision","Deep Learning","Event Based Data"],"github":"https://gitlab.com/neuronics/n-ear","external":"https://gitlab.com/neuronics/n-ear"},"html":"<p>We present a class of efficient model called n-EAR for on-board vehicle activity recognition on autonomous vehicles. The core decision making in a self driving car relies heavily on the relative position of the vehicle with respect to its surroundings. We introduce two novel techniques, first an event based attention sampling technique that leverages on the bio-inspired event data to adaptively sample the frame-based data. Secondly, a two-stream architecture that that efficiently trade off between latency and accuracy.</p>"}},{"node":{"frontmatter":{"title":"Neuro Electronic Hybrid System","tech":["Signal Processing","Neuro Electrics"],"github":"https://gitlab.com/neuronics/neuro-electronic-hybrid-system","external":"https://gitlab.com/neuronics/neuro-electronic-hybrid-system"},"html":"<p>Neuro-electronic hybrid systems have been gaining interest of researchers as a possible architecture for computing. This aims to exploit the strengths of biological neuronal systems with their immense parallel processing and learning capabilities along with that of VLSI systems. Towards this end, we have set up a system which demonstrates the use of a live neuronal culture to solve a real world problem of classification of natural audio/visual signals.</p>"}},{"node":{"frontmatter":{"title":"N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces","tech":["Event Based Neurmorphic Vision","Computer Vision","Deep Learning","Human Activity Recognition"],"github":"https://gitlab.com/neuronics/n-har","external":"https://gitlab.com/neuronics/n-har"},"html":"<p>N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces.</p>\n<p>A system to achieve the task of human activity recognition based on the event-based camera data. We achieved a SOTA accuracy of 94.3% using event memory surfaces on our activity recognition dataset.</p>"}},{"node":{"frontmatter":{"title":"System and Method for exhale controlled Augmentative and Assistive Communication device for communication and controlling IOT device","tech":["Singal Processing","Arduino","Algolia"],"github":"","external":""},"html":"<ul>\n<li><i>Update patent link </i></li>\n</ul>"}},{"node":{"frontmatter":{"title":"Synthetic Data from CARLA Simulator for Vision Tasks ","tech":["Python","CARLA API","UnReal Engine","C,C++"],"github":"","external":"https://carla.org/"},"html":"<ul>\n<li><i>Will be updated soon</i></li>\n</ul>"}},{"node":{"frontmatter":{"title":"Anomaly Detection on Neuromorphic Data ","tech":["Deep Learning","Computer Vision","Event Base Data"],"github":"","external":""},"html":"<ul>\n<li>Guide: Dr. Chetan Singh Thakur and Dr. Anirban Chakraborty  [Built as a part of Brain, Computing and Learning Workshop 2017]</li>\n<li>Developed a deep learning neural network to perform anomaly on UCF Video Dataset</li>\n<li>Best project of the workshop</li>\n</ul>"}}]}}}