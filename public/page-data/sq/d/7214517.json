{"data":{"jobs":{"edges":[{"node":{"frontmatter":{"title":"A Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware","company":"ICIP 2021","location":null,"range":null,"url":null},"html":"<ul>\n<li>\n<p><a href=\"\">Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware</a></p>\n<ul>\n<li>Object detection and localization can be possible directly in the Compressed Domain (easily upto 20x compression).</li>\n<li>Achieved SOTA 46.27% mAP(Mean Average Precision) on a GeForce GTX 1080 Ti with an inference time of 23ms.</li>\n<li>Deployed on a NVIDIA TX2 embedded board with 45.11% mAP with an inference time of 34ms.</li>\n</ul>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces","company":"ISCAS 2019","location":null,"range":null,"url":null},"html":"<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/8702581\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces</a></p>\n<ul>\n<li>We used eventmemory surfaces to make the sparse event data compatible withdeep  convolutional  neural  networks  (CNNs).  We  leverage  uponthe recent advances in deep convolutional networks based videoanalysis  and  adapt  such  frameworks  onto  the  neuromorphicdomain.  We  also  provide  the  community  with  a  new  datasetconsisting of five categories of human activities captured in realworld  without  any  simulations.  We  achieved  an  accuracy  of94.3%  using  event  memory  surfaces  on  our  activity  recognitiondataset.</li>\n</ul>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"Live Demonstration: Real-Time Implementation of Proto-Object Based Visual Saliency Model","company":"ISCAS 2019","location":null,"range":null,"url":null},"html":"<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/abstract/document/8702200\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Real-time Implementation of Proto-Object Based Visual Saliency Model</a></p>\n<ul>\n<li>Our  current  implementation  considers  the  dynamic  temporal  motion change  by  convoluting  along  time  efficiently  by  parallelly processing  them.  We  have  implemented  the  model  on  an NVIDIA  Jetson  TX1  board,  which  has  NVIDIAMaxwell  GPU  with  256  NVIDIA  CUDA  Cores, hosted  onan  Ubuntu  environment.  The  board  has  a  5  MP  fixed  focus MIPI  CSI  camera  through  which  the  frames  are  fetched using a Quad-core ARM Cortex-A57 MPCore Processor with 4 GB LPDDR4  Memory.</li>\n</ul>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"A Compressive Sensing Video dataset using Pixel-wise coded exposure","company":"arXiv","location":null,"range":null,"url":null},"html":"<ul>\n<li>\n<p><a href=\"https://arxiv.org/abs/1905.10054\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">A Compressive Sensing Video dataset using Pixel-wise coded exposure</a></p>\n<ul>\n<li>Object detection and localization can be possible directly in the Compressed Domain (easily upto 20x compression).</li>\n<li>Achieved SOTA 46.27% mAP(Mean Average Precision) on a GeForce GTX 1080 Ti with an inference time of 23ms.</li>\n<li>Deployed on a NVIDIA TX2 embedded board with 45.11% mAP with an inference time of 34ms.</li>\n</ul>\n</li>\n</ul>"}},{"node":{"frontmatter":{"title":"System and method for real-time object detection and localization in compressive sensed video on embedded hardware","company":"IN Patent App. KNS.IES.370IN1","location":null,"range":null,"url":null},"html":"<p><a href=\"\">Real-Time Object Detection and Localization in Compressive Sensed Video on Embedded Hardware</a></p>\n<ul>\n<li>Object detection and localization can be possible directly in the Compressed Domain (easily upto 20x compression).</li>\n<li>Achieved SOTA 46.27% mAP(Mean Average Precision) on a GeForce GTX 1080 Ti with an inference time of 23ms.</li>\n<li>Deployed on a NVIDIA TX2 embedded board with 45.11% mAP with an inference time of 34ms.</li>\n</ul>"}},{"node":{"frontmatter":{"title":"System and Method for exhale controlled Augmentativeand Assistive Communicationdevice for communication and controlling IOT device","company":"IN Patent App. 201641044496","location":null,"range":null,"url":null},"html":"<p><a href=\"\">System and Method for exhale controlled Augmentativeand Assistive Communicationdevice for communication and controlling IOT device</a></p>"}},{"node":{"frontmatter":{"title":"System and method for ego-centric activity recognitionfrom vehicle on-boardneuromorphic cameras","company":"IN Patent App. KNS.IES.1281IN1","location":null,"range":null,"url":null},"html":"<p><a href=\"\">n-EAR</a></p>"}}]}}}