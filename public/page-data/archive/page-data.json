{"componentChunkName":"component---src-pages-archive-js","path":"/archive/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"frontmatter":{"date":"2020-09-15","title":"Anomaly Detection on Neuromorphic Data ","github":"","external":"","company":"IISc"},"html":"<ul>\n<li>Guide: Dr. Chetan Singh Thakur and Dr. Anirban Chakraborty  [Built as a part of Brain, Computing and Learning Workshop 2017]</li>\n<li>Developed a deep learning neural network to perform anomaly on UCF Video Dataset</li>\n<li>Best project of the workshop</li>\n</ul>"}},{"node":{"frontmatter":{"date":"2020-08-02","title":"Synthetic Data from CARLA Simulator for Vision Tasks ","github":"","external":"https://carla.org/","company":"CARLA"},"html":"<ul>\n<li><i>Will be updated soon</i></li>\n</ul>"}},{"node":{"frontmatter":{"date":"2018-12-01","title":"System and Method for exhale controlled Augmentative and Assistive Communication device for communication and controlling IOT device","github":"","external":"","company":"MIT"},"html":"<ul>\n<li><i>Update patent link </i></li>\n</ul>"}},{"node":{"frontmatter":{"date":"2018-10-01","title":"N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces","github":"https://gitlab.com/neuronics/n-har","external":"https://gitlab.com/neuronics/n-har","company":null},"html":"<p>N-HAR: A Neuromorphic Event-Based Human Activity Recognition System using Memory Surfaces.</p>\n<p>A system to achieve the task of human activity recognition based on the event-based camera data. We achieved a SOTA accuracy of 94.3% using event memory surfaces on our activity recognition dataset.</p>"}},{"node":{"frontmatter":{"date":"2018-05-01","title":"Neuro Electronic Hybrid System","github":"https://gitlab.com/neuronics/neuro-electronic-hybrid-system","external":"https://gitlab.com/neuronics/neuro-electronic-hybrid-system","company":"multichannelsystems"},"html":"<p>Neuro-electronic hybrid systems have been gaining interest of researchers as a possible architecture for computing. This aims to exploit the strengths of biological neuronal systems with their immense parallel processing and learning capabilities along with that of VLSI systems. Towards this end, we have set up a system which demonstrates the use of a live neuronal culture to solve a real world problem of classification of natural audio/visual signals.</p>"}},{"node":{"frontmatter":{"date":"2017-12-01","title":"n-EAR: Neuromorphic Ego motion vehicle Activity Recognition using onboard cameras","github":"https://gitlab.com/neuronics/n-ear","external":"https://gitlab.com/neuronics/n-ear","company":"Wirpo"},"html":"<p>We present a class of efficient model called n-EAR for on-board vehicle activity recognition on autonomous vehicles. The core decision making in a self driving car relies heavily on the relative position of the vehicle with respect to its surroundings. We introduce two novel techniques, first an event based attention sampling technique that leverages on the bio-inspired event data to adaptively sample the frame-based data. Secondly, a two-stream architecture that that efficiently trade off between latency and accuracy.</p>"}},{"node":{"frontmatter":{"date":"2017-11-01","title":"Neuromprhic Event Based ATIS Camera","github":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/ATIS","external":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/ATIS","company":"Prophesee"},"html":"<p>REVEALS THE INVISIBLE</p>\n<p>Inspired by human vision and built on the foundation of neuromorphic engineering. PROPHESEE is the revolutionary system that gives Metavision to machines, revealing what was previously invisible to them. Capturing hyper fast and fleeting scene dynamics</p>\n<ul>\n<li>\n<p>greater than 10000 fps (equivalent temporal precision)</p>\n</li>\n<li>\n<p>Managing extreme lighting conditions</p>\n</li>\n<li>\n<p>120 dB dynamic range</p>\n</li>\n<li>\n<p>Enabling new levels of power efficiency</p>\n</li>\n<li>\n<p>&#x3C; 10mW</p>\n</li>\n</ul>"}},{"node":{"frontmatter":{"date":"2017-11-01","title":"DAVIS Neuromprhic Event Based Camera","github":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/DAVIS","external":"https://gitlab.com/neuronics/NeuromophicVision/-/tree/master/DAVIS","company":"Prophesee"},"html":"<p>inivation DAVIS\nNeuromorphically inspired camera, that is based on combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is > 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at > 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum 2 pixels with 9.4% fill factor.</p>"}},{"node":{"frontmatter":{"date":"2017-11-01","title":"A Compressive Sensing Video dataset using Pixel-wise coded exposure","github":"https://arxiv.org/abs/1905.10054","external":"https://arxiv.org/abs/1905.10054","company":"Prophesee"},"html":"<p>A Compressive Sensing Video dataset using Pixel-wise coded exposure</p>"}},{"node":{"frontmatter":{"date":"2017-04-03","title":"Live Demonstration: Real-Time Implementation of Proto-Object Based Visual Saliency Model","github":"https://github.com/satabios/visual_saliency_TX2","external":"https://github.com/satabios/visual_saliency_TX2","company":null},"html":"<p>Real-Time Implementation of Proto-Object Based Visual Saliency</p>\n<ul>\n<li>We demonstrate a real-time implementation of a proto sobject based neuromorphic visual saliency model</li>\n</ul>"}}]}},"pageContext":{}},"staticQueryHashes":["1994492073","2461614994","2585295041","604461682","613114584","7214517"]}